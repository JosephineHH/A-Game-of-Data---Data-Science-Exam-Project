---
title: "Profanity time series modelling"
author: "Josephine Hillebrand Hansen"
date: "19/05/2020"
output: html_document
---


Setting up and adding a column to the dataset that allows us to measure the number of times swear words were said during each episode

```{r setup, include=FALSE}
data = read.csv("got_cleaned.csv")
library(sjmisc)

#Create empty profanity column
data[,"profanity"] <- NA

profanity = read.csv("Google-profanity-words-master/list.txt", header = FALSE)


#swear_words = vector(mode = "list", length = length(data$lemma))


for (i in 1:length(data$lemma)){
  sentence = data$lemma[i]
  sw = 0
  
  for (prof in profanity$V1){
    nsw = str_count(sentence, prof)
    sw = sw + nsw
    sentence = gsub(prof, "", sentence)
    }
  data$profanity[i] = sw
  print(i)
}

write.csv(data, "got_profanity_data.csv")

data = read.csv("got_profanity_data.csv")


#Creating two new df with profanity count for each episode, as well as for each season
library(dplyr)
prof_data_episode = data %>%
  group_by(N_serie) %>%
  summarize(sum_profanity = sum(profanity))
  

prof_data_season = data %>%
  group_by(Season) %>%
  summarize(sum_profanity = sum(profanity))
  


```



```{r}
library(fpp2)

ts_prof = ts(prof_data_episode$sum_profanity, frequency = 10)

autoplot(ts_prof) +
  ggtitle("Number of swear words per episode") +
  xlab("Episode") +
  ylab("Number of swear words")


#Season
ts_prof_season = ts(prof_data_season$sum_profanity)

autoplot(ts_prof_season) +
  ggtitle("Number of swear words per season") +
  xlab("Season") +
  ylab("Number of swear words")



```



#Creating the corrected data - sentences containing swear words

```{r}
prof_data_episode_ratio = data %>%
  group_by(N_serie) %>%
  summarize(sum_profanity = sum(profanity)/length(N_serie))

ts_prof_ratio = ts(prof_data_episode_ratio$sum_profanity, frequency = 10)
autoplot(ts_prof_ratio) +
  ggtitle("Proportion of profanities") +
  xlab("Season") +
  ylab("Proportion of profanities")



prof_data_season_ratio = data %>%
  group_by(Season) %>%
  summarize(sum_profanity = sum(profanity)/length(Season))

  
ts_prof_season_ratio = ts(prof_data_season_ratio$sum_profanity)

autoplot(ts_prof_season_ratio) +
  ggtitle("Number of swear words per season corrected for number of sentences") +
  xlab("Season") +
  ylab("Number of swear words")


```


#Lag plots
```{r}
gglagplot(ts_prof,
          lags = 20,
          seasonal = TRUE)




gglagplot(ts_prof_ratio,
          lags = 20,
          seasonal = TRUE)
```

#Autocorrelation plots
```{r}
ggAcf(ts_prof, lag.max = 60)

ggAcf(ts_prof_ratio, lag.max = 30) +
  ggtitle("Autocorrelation plot of proportion of profanities")

ggAcf(ts_prof_season)

ggAcf(ts_prof_season_ratio)
```




#Simple forecasting
```{r}
# Plot some forecasts - Seasonal for each episodes
autoplot(ts_prof_ratio) +
  autolayer(meanf(ts_prof_ratio, h=11),
    series="Mean", PI=FALSE) +
  autolayer(naive(ts_prof_ratio, h=11),
    series="Na誰ve", PI=FALSE) +
  autolayer(snaive(ts_prof_ratio, h=11),
    series="Seasonal na誰ve", PI=FALSE) +
  autolayer(rwf(ts_prof_ratio, drift=TRUE, h=10),
    series="Drift", PI=FALSE) +
  ggtitle("Forecasts for profanity in future episodes") +
  xlab("Season") + ylab("Swear words") +
  guides(colour=guide_legend(title="Forecast"))



#Non-seasonal season
autoplot(ts_prof_season_ratio) +
  autolayer(meanf(ts_prof_season_ratio, h=5),
    series="Mean", PI=FALSE) +
  autolayer(rwf(ts_prof_season_ratio, h=5),
    series="Na誰ve", PI=FALSE) +
  autolayer(rwf(ts_prof_season_ratio, drift=TRUE, h=5),
    series="Drift", PI=FALSE) +
  ggtitle("Simple forecasting of profanity in GOT") +
  xlab("Season") + ylab("Profanity count") +
  guides(colour=guide_legend(title="Forecast"))



```


#Residuals from simple forecasting methods
```{r}
res <- residuals(naive(ts_prof_ratio))
autoplot(res) + xlab("Day") + ylab("") +
  ggtitle("Residuals from na誰ve method")

mean(res[-1]) #This is zero, thus it is not biased

gghistogram(res) + ggtitle("Histogram of residuals")

ggAcf(res) + ggtitle("ACF of residuals")


#Portmanteau tests for autocorrelation
Box.test(res, lag = 10, fitdf = 0)
#this is significant, which indicates that there is still some information left in the residuals not captured by the naive method


checkresiduals(naive(ts_prof_ratio))
checkresiduals(snaive(ts_prof_ratio))
checkresiduals(rwf(ts_prof_ratio, drift = TRUE))
checkresiduals(meanf(ts_prof_ratio))


checkresiduals(naive(ts_prof_season_ratio))
checkresiduals(rwf(ts_prof_season_ratio, drift = TRUE))
checkresiduals(meanf(ts_prof_season_ratio))





```





#Time series cross validation
```{r}
e <- tsCV(ts_prof_ratio, rwf, drift=TRUE, h=1)
sqrt(mean(e^2, na.rm=TRUE))

sqrt(mean(residuals(rwf(ts_prof_ratio, drift=TRUE))^2, na.rm=TRUE))

```




#Forecasting 1-8 steps into the future
```{r}

#Naive methods
e <- tsCV(ts_prof_ratio, forecastfunction=naive, h=8)
# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = T)
# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point() #Best two episodes into the future


e <- tsCV(ts_prof_season_ratio, forecastfunction=naive, h=5)
# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = T)
# Plot the MSE values against the forecast horizon
data.frame(h = 1:5, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()





#Mean

e <- tsCV(ts_prof_ratio, forecastfunction=meanf, h=8)
# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = T)
# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point() #Best two episodes into the future


e <- tsCV(ts_prof_season_ratio, forecastfunction=meanf, h=8)
# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = T)
# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()



#Snaive - seasonal naive
e <- tsCV(ts_prof_ratio, forecastfunction=snaive, h=8)
# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = T)
# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point() #Best two episodes into the future

```


This doesn't work
#RWF - DRIFT
e <- tsCV(ts_prof_ratio, forecastfunction=rwf, drift = TRUE, h=8)
# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = T)
# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point() #Best two episodes into the future


e <- tsCV(ts_prof_season_ratio, forecastfunction=rwf, h=8)
# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = T)
# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()





#Plotting prediction intervals
```{r}
autoplot(naive(ts_prof_ratio))
autoplot(meanf(ts_prof_ratio))
autoplot(snaive(ts_prof_ratio))
autoplot(rwf(ts_prof_ratio, drift = TRUE))



autoplot(naive(ts_prof_season_ratio))
autoplot(meanf(ts_prof_season_ratio))
autoplot(rwf(ts_prof_season_ratio, drift = TRUE))

```





#Creating a linear model
```{r}
#Fitting a linear model to our data

fit.ts_prof_ratio <- tslm(ts_prof_ratio ~ trend)
summary(fit.ts_prof_ratio)

#Linear model with effect of season
fit.ts_prof_ratio_season <- tslm(ts_prof_ratio ~ trend + season)
summary(fit.ts_prof_ratio_season)

fit.ts_season <- tslm(ts_prof_ratio ~ season)

AIC(fit.ts_prof_ratio, fit.ts_prof_ratio_season, fit.ts_season)





#on the data from season
fit.ts_prof_season_ratio <- tslm(ts_prof_season_ratio ~ trend)
summary(fit.ts_prof_season_ratio)

```


```{r}

ts_prof_ratio = ts(prof_data_episode_ratio$sum_profanity)

h <- 10
fit.lin <- tslm(ts_prof_ratio ~ trend)
fcasts.lin <- forecast(fit.lin, h = h)
fit.exp <- tslm(ts_prof_ratio ~ trend, lambda = 0)
fcasts.exp <- forecast(fit.exp, h = h)

t <- time(ts_prof_ratio)
t.break1 <- 56
tb1 <- ts(pmax(0, t - t.break1), start = 0)

fit.pw <- tslm(ts_prof_ratio ~ t + tb1)
t.new <- t[length(t)] + seq(h)
tb1.new <- tb1[length(tb1)] + seq(h)

newdata <- cbind(t=t.new, tb1=tb1.new) %>%
  as.data.frame()
fcasts.pw <- forecast(fit.pw, newdata = newdata)

fit.spline <- tslm(ts_prof_ratio ~ t + I(t^2) + I(t^3) +
  I(tb1^3))#, lambda = 0)
fcasts.spl <- forecast(fit.spline, newdata = newdata)

autoplot(ts_prof_ratio) +
  #autolayer(fitted(fit.lin), series = "Linear") +
  #autolayer(fitted(fit.exp), series = "Exponential") +
  #autolayer(fitted(fit.pw), series = "Piecewise") +
  autolayer(fitted(fit.spline), series = "Cubic Spline") +
  #autolayer(fcasts.pw, series="Piecewise", alpha = 0.5) +
  #autolayer(fcasts.lin, series="Linear", PI=FALSE) +
  #autolayer(fcasts.exp, series="Exponential", PI=FALSE) +
  autolayer(fcasts.spl, series="Cubic Spline", alpha = 0.5) + # PI=FALSE) +
  xlab("Episode") + ylab("Proportion of swear words") +
  ggtitle("Fit of predictions of swear-word proportions in GOT") +
  guides(colour = guide_legend(title = " "))
```



#Decompositing our time series
```{r}

ts_prof_ratio = ts(prof_data_episode_ratio$sum_profanity, frequency = 10)

ts_prof_ratio %>% decompose(type="additive") %>%
  autoplot() + xlab("Season") +
  ggtitle("Classical Additive Decomposition
    of Proportion of Profanities")

ts_prof_ratio %>% decompose(type="additive") -> fit

autoplot(fit) +
  ggtitle("Classical multiplicative decomposition
    of profanity in GOT episodes")



autoplot(ts_prof_ratio, series="Data") +
  autolayer(trendcycle(fit), series="Trend") +
  autolayer(seasadj(fit), series="Seasonally Adjusted") +
  xlab("Season") + ylab("Proportion of profanities") +
  ggtitle("Seasonally Adjusted Profanity Data") +
  scale_colour_manual(values=c("gray","blue","red"),
             breaks=c("Data","Seasonally Adjusted","Trend"))



```




#Exponential smoothing
```{r}

# Estimate parameters
fc <- ses(ts_prof_ratio, h=5)
# Accuracy of one-step-ahead training errors
round(accuracy(fc),2)


autoplot(fc) +
  autolayer(fitted(fc), series="Fitted") +
  ylab("Profanity ratio") + xlab("Season")

summary(fc)




#Holst method + damped holst method
fc <- holt(ts_prof_ratio, h=15)
fc2 <- holt(ts_prof_ratio, damped=TRUE, phi = 0.9, h=15)
autoplot(ts_prof_ratio) +
  autolayer(fc, series="Holt's method", PI=FALSE) +
  autolayer(fc2, series="Damped Holt's method", PI=FALSE) +
  ggtitle("Forecasts from Holt's method") + xlab("Year") +
  ylab("Air passengers in Australia (millions)") +
  guides(colour=guide_legend(title="Forecast"))




```





#ARIMA
Models that are neither seasonal nor have a trend
```{r}
Box.test(ts_prof_ratio)
ggAcf(ts_prof_ratio)

Box.test(diff(ts_prof_ratio))
ggAcf(diff(ts_prof_ratio))

#We keep the non-differentiated time-series

#Plot to compare differenced and non-differenced data (Seasonal difference)
cbind("Billion kWh" = ts_prof_ratio,
      "Logs" = log(ts_prof_ratio),
      "Seasonally\n differenced logs" =
        diff(log(ts_prof_ratio),12),
      "Doubly\n differenced logs" =
        diff(diff(log(ts_prof_ratio),12),1)) %>%
  autoplot(facets=TRUE) +
    xlab("Season") + ylab("") +
    ggtitle("Profanity in GOT")


#Is differencing required - i.e. is our data stationary
library(urca)
ts_prof_ratio %>% ur.kpss() %>% summary() #Statinary (below 5 % critical value)
ts_prof_ratio %>% diff() %>% ur.kpss() %>% summary()


ndiffs(ts_prof_ratio) #This test reveals that 1 differentiating is required

ts_prof_ratio %>% diff() %>% ur.kpss() %>% summary() #New test


#Is seasonal differntiating required
nsdiffs(ts_prof_ratio)



```


#Autoarima
```{r}
fit <- auto.arima(ts_prof_ratio, seasonal=FALSE)
summary(fit)
fit %>% forecast(h=10) %>% autoplot(include=80)



#Differentiating
#An autoarima that works harder
fit2 <- auto.arima(diff(ts_prof_ratio), seasonal = FALSE, stepwise = FALSE, approximation = FALSE)
summary(fit2)
fit2 %>% forecast(h=10) %>% autoplot(include=80)

#(p,d,q)
#(0,1,1) - no constant

ggAcf(diff(ts_prof_ratio))
ggPacf(diff(ts_prof_ratio))


#Check residuals of our fit
checkresiduals(fit2)
```


Test of variance
```{r}
lambda <- BoxCox.lambda(ts_prof_ratio) # = 0.27
lambda # if lambda was around 1, then you do not need any power transform
new_ts <- BoxCox(ts_prof_ratio,lambda)


MTS::archTest(ts_prof_ratio, lag = 10)
```





#CHRIS
```{r}
ggtsdisplay(ts_prof_ratio)
```








#New plots - comparing models
```{r}
#As there is no seasonality, we redefine our time series with frequency = 1
ts_p = ts(ts_prof_ratio, frequency = 1)
#differenced ts for ARIMA
ts_diff = diff(ts_p)



h <- 10
#Linear
fit.lin <- tslm(ts_p ~ trend)
fcasts.lin <- forecast(fit.lin, h = h)
#Exponential
fit.exp <- tslm(ts_p ~ trend)
fcasts.exp <- forecast(fit.exp, h = h)

#Piecewise
t <- time(ts_p)
t.break1 <- 60
tb1 <- ts(pmax(0, t - t.break1), start = 0)

fit.pw <- tslm(ts_p ~ t + tb1)
t.new <- t[length(t)] + seq(h)
tb1.new <- tb1[length(tb1)] + seq(h)

newdata <- cbind(t=t.new, tb1=tb1.new) %>%
  as.data.frame()
fcasts.pw <- forecast(fit.pw, newdata = newdata)

#cubic spline
fit.spline <- tslm(ts_p ~ t + I(t^2) + I(t^3) +
  I(tb1^3), lambda = 0)
fcasts.spl <- forecast(fit.spline, newdata = newdata)

fit.arima <- auto.arima(ts_diff, seasonal = FALSE, stepwise = FALSE, approximation = FALSE)
fcasts.arima <- forecast(fit.arima, h=h)

fit.mean <- meanf(ts_p)
fcasts.mean <- forecast(fit.mean, h=h)
fit.naive <- naive(ts_p)
fcasts.naive <- forecast(fit.naive, h=h)
fit.drift <- rwf(ts_p, drift = TRUE)
fcasts.drift <- forecast(fit.drift, h = h)


round(accuracy(fcasts.lin),2)
round(accuracy(fcasts.exp),2)
round(accuracy(fcasts.pw),2)
round(accuracy(fcasts.spl),2)
round(accuracy(fcasts.arima),2)
round(accuracy(fcasts.mean),2)
round(accuracy(fcasts.naive),2)
round(accuracy(fcasts.drift),2)


```

Cross validation
```{r}
#trend
flm1 <- function(y, h) { forecast(tslm(y ~ trend), h=h) }
trend_p <- tsCV(ts_p, flm1, h=6)
#trend
flm2 <- function(y, h) { forecast(tslm(y ~ trend, lambda=0), h=h) }
exp_p <- tsCV(ts_p, flm2, h=6)
#sqrt(mean(e^2, na.rm=TRUE))
#Our basic-bitch forecasting functions
mean_p <- tsCV(ts_p, forecastfunction=meanf, h=8)
naive_p <- tsCV(ts_p, forecastfunction=naive, h=8)
#drift_p <- tsCV(ts_p, forecastfunction=rwf, drift = TRUE, h=8))

farima <- function(x, h) {
  forecast(auto.arima(x), h=h)
}
# Compute CV errors for ARIMA as e2
arima_p <- tsCV(ts_diff, farima, h=1)
# Find MSE of each model class
mean(e2^2, na.rm=TRUE)



#Piecewise
fpw <- function(x, h) {
  t <- time(x)
  if (t > 50){
    t.break1 <- 60}
  else{
      t.break1 = 0}
  tb1 <- ts(pmax(0, t - t.break1), start = 0)
  
  fit.pw <- tslm(x ~ t + tb1)
  t.new <- t[length(t)] + seq(h)
  tb1.new <- tb1[length(tb1)] + seq(h)

  newdata <- cbind(t=t.new, tb1=tb1.new) %>%
  as.data.frame()
  
  forecast(fit.pw, h=h)
}



mean(arima_p^2, na.rm=TRUE)
mean(mean_p^2, na.rm=TRUE)
mean(naive_p^2, na.rm=TRUE)
mean(exp_p^2, na.rm=TRUE)
mean(arima_p^2, na.rm=TRUE)
mean(arima_p^2, na.rm=TRUE)

```
