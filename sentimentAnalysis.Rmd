---
title: "GOT"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)



library(readr);library(textclean);library(pacman);library(fpp2)
data <-read_delim("got_cleaned.csv",",", escape_double = FALSE, trim_ws = TRUE)


```

```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(sentimentr, dplyr, magrittr)


mytext <- data$lemma

mytext <- get_sentences(mytext)
sent_scor <- as.data.frame(sentiment(mytext))

data1 <- merge(data,sent_scor, by.x='Column1', by.y='element_id')
```

```{r}
#subsetting
length_ep = data1 %>%
  group_by(data1$Episode)%>%
  summarise(length(Episode))

DW <- as.data.frame(subset(data1, Episode == 'dark wings dark words'))

```

```{r}
# plot time series

ts_DW <- ts(DW$sentiment)

autoplot(ts_DW) +
  ggtitle("Sentiment scores per line") +
  xlab("Line") +
  ylab("Sentiment Score")



```


```{r}
# Moving average
ma(ts_DW, 5)

autoplot(ts_DW, series="Data") +
  autolayer(ma(ts_DW,10), series="5-MA") +
  xlab("Line") + ylab("sentiment score") +
  ggtitle("Sentiment score per line, MA smoothed") +
  scale_colour_manual(values=c("Data"="grey50","5-MA"="red"),
                      breaks=c("Data","5-MA"))

```

```{r}
# simple exponential smoothing

# Estimate parameters
DW_ses <- ses(ts_DW, h=2)
# Accuracy of one-step-ahead training errors
round(accuracy(DW_ses),2)

autoplot(DW_ses) +
  autolayer(fitted(DW_ses), series="Fitted") +
  ylab("Oil (millions of tonnes)") + xlab("Year")

summary(DW_ses)
```


```{r}

# autocorrelation plots
ggAcf(ts_DW)

#
```

simple forecasting methods
```{r}


#Non-seasonal 
autoplot(ts_DW) +
  autolayer(meanf(ts_DW, h=50),
    series="Mean", PI=FALSE) +
  autolayer(rwf(ts_DW, h=50),
    series="Naïve", PI=FALSE) +
  autolayer(rwf(ts_DW, drift=TRUE, h=50),
    series="Drift", PI=FALSE) +
  ggtitle("Simple forecasting of profanity in GOT") +
  xlab("Season") + ylab("Profanity count") +
  guides(colour=guide_legend(title="Forecast"))


```


Group by episode

```{r}
library(dplyr)
sent_episode = data1 %>%
  group_by(N_serie) %>%
  summarise(mean_sent = mean(sentiment))
  

ts_sent_ep <- ts(sent_episode$mean_sent, frequency = 10)

autoplot(ts_sent_ep) +
  ggtitle("Sentiment scores per line") +
  xlab("Line") +
  ylab("Sentiment Score")

```


```{r}
ggAcf(ts_sent_ep)
```

```{r}
autoplot(ts_sent_ep) +
  autolayer(meanf(ts_sent_ep, h=5),
    series="Mean", PI=FALSE) +
  autolayer(rwf(ts_sent_ep, h=5),
    series="Naïve", PI=FALSE) +
  autolayer(snaive(ts_sent_ep, h=5),
            series = "Seasonal Naive", PI = FALSE)+
  autolayer(rwf(ts_sent_ep, drift=TRUE, h=5),
    series="Drift", PI=FALSE) +
  ggtitle("Simple forecasting of profanity in GOT") +
  xlab("Season") + ylab("Profanity count") +
  guides(colour=guide_legend(title="Forecast"))

checkresiduals(naive(ts_sent_ep))
checkresiduals(rwf(ts_sent_ep, drift = TRUE))
checkresiduals(meanf(ts_sent_ep))
checkresiduals(snaive(ts_sent_ep))




```

# decomposition
```{r}
ts_sent_ep %>% decompose(type="additive") %>%
  autoplot() + xlab("Year") +
  ggtitle("Classical multiplicative decomposition
    of electrical equipment index")
```
we cna conclude that there is no patterns in the sentiment across episodes. 

```{r}

ts_sent_ep = ts(sent_episode$mean_sent)

h <- 10
fit.lin <- tslm(ts_sent_ep ~ trend)
fcasts.lin <- forecast(fit.lin, h = h)
fit.exp <- tslm(ts_sent_ep ~ trend, lambda = 0)
fcasts.exp <- forecast(fit.exp, h = h)

t <- time(ts_sent_ep)
t.break1 <- 10
t.break2 <- 20
t.break3 <- 30
t.break4 <- 40
t.break5 <- 50
t.break6 <- 60

tb1 <- ts(pmax(0, t - t.break1), start = 0)
tb2 <- ts(pmax(0, t - t.break2), start = 0)
tb3 <- ts(pmax(0, t - t.break3), start = 0)
tb4 <- ts(pmax(0, t - t.break4), start = 0)
tb5 <- ts(pmax(0, t - t.break5), start = 0)
tb6 <- ts(pmax(0, t - t.break6), start = 0)

fit.pw <- tslm(ts_sent_ep ~ t + tb1+tb2+tb3+tb4+tb5+tb6)
t.new <- t[length(t)] + seq(h)
tb1.new <- tb1[length(tb1)] + seq(h)
tb2.new <- tb2[length(tb2)] + seq(h)
tb3.new <- tb3[length(tb3)] + seq(h)
tb4.new <- tb4[length(tb4)] + seq(h)
tb5.new <- tb5[length(tb5)] + seq(h)
tb6.new <- tb6[length(tb6)] + seq(h)

newdata <- cbind(t=t.new, tb1=tb1.new,tb2=tb2.new,tb3=tb3.new,tb4=tb4.new,tb5=tb5.new,tb6=tb6.new) %>%
  as.data.frame()
fcasts.pw <- forecast(fit.pw, newdata = newdata)

fit.spline <- tslm(ts_sent_ep ~ t + I(t^2) + I(t^3) +
  I(tb1^3)+ I(tb2^3)+ I(tb3^3)+ I(tb4^3)+ I(tb5^3)+ I(tb6^3))
fcasts.spl <- forecast(fit.spline, newdata = newdata)

autoplot(ts_sent_ep) +
  autolayer(fitted(fit.lin), series = "Linear") +
  autolayer(fitted(fit.exp), series = "Exponential") +
  autolayer(fitted(fit.pw), series = "Piecewise") +
  #autolayer(fitted(fit.spline), series = "Cubic Spline") +
  autolayer(fcasts.pw, series="Piecewise", alpha = 0.5) +
  autolayer(fcasts.lin, series="Linear", PI=FALSE) +
  autolayer(fcasts.exp, series="Exponential", PI=FALSE) +
  #autolayer(fcasts.spl, series="Cubic Spline", alpha = 0.5) + # PI=FALSE) +
  xlab("Episode") + ylab("Sentiment Score") +
  ggtitle("Fit of predictions of swear-word proportions in GOT") +
  guides(colour = guide_legend(title = " "))
```



```{r}



h <- 10
fit.lin <- tslm(ts_sent_ep ~ trend)
fcasts.lin <- forecast(fit.lin, h = h)
fit.exp <- tslm(ts_sent_ep ~ trend, lambda = 0)
fcasts.exp <- forecast(fit.exp, h = h)

t <- time(ts_sent_ep)
t.break1 <- 50
tb1 <- ts(pmax(0, t - t.break1), start = 0)

fit.pw <- tslm(ts_sent_ep ~ t + tb1)
t.new <- t[length(t)] + seq(h)
tb1.new <- tb1[length(tb1)] + seq(h)

newdata <- cbind(t=t.new, tb1=tb1.new) %>%
  as.data.frame()
fcasts.pw <- forecast(fit.pw, newdata = newdata)

fit.spline <- tslm(ts_sent_ep ~ t + I(t^2) + I(t^3) +
  I(tb1^3))
fcasts.spl <- forecast(fit.spline, newdata = newdata)

autoplot(ts_sent_ep) +
  autolayer(fitted(fit.lin), series = "Linear") +
  autolayer(fitted(fit.exp), series = "Exponential") +
  autolayer(fitted(fit.pw), series = "Piecewise") +
 # autolayer(fitted(fit.spline), series = "Cubic Spline") +
  autolayer(fcasts.pw, series="Piecewise", alpha = 0.5) +
  autolayer(fcasts.lin, series="Linear", PI=FALSE) +
  autolayer(fcasts.exp, series="Exponential", PI=FALSE) +
  #autolayer(fcasts.spl, series="Cubic Spline", alpha = 0.5) + # PI=FALSE) +
  xlab("Episode") + ylab("Proportion of swear words") +
  ggtitle("Fit of predictions of swear-word proportions in GOT") +
  guides(colour = guide_legend(title = " "))

```

## Validating forecasting of season 8

```{r}
# sentiment score for all seasons

data2 <-read_delim("got_season8_cleaned.csv",",", escape_double = FALSE, trim_ws = TRUE)

mytext <- data2$lemma

mytext <- get_sentences(mytext)
sent_scor <- as.data.frame(sentiment(mytext))

data2$Column1 <- seq.int(nrow(data2))

S8 <- merge(data2,sent_scor, by.x='Column1', by.y='element_id')

library(dplyr);library(tidyverse)
sent_episode_s8 = S8 %>%
  group_by(Episode) %>%
  summarise(mean_sent = mean(sentiment))
  
sent_episode_s8$Episode <- c(68,69,70,71,72,73)

sent_episode_s8 <- rename(sent_episode_s8, N_serie = Episode)

# appending season 8 to rest of data
sent_episode_all <- rbind(sent_episode, sent_episode_s8)
```


```{r}
ts_all <- ts(sent_episode_all$mean_sent)


```




```{r}


h <- 10
fit.lin <- tslm(ts_sent_ep ~ trend)
fcasts.lin <- forecast(fit.lin, h = h)
fit.exp <- tslm(ts_sent_ep ~ trend, lambda = 0)
fcasts.exp <- forecast(fit.exp, h = h)

t <- time(ts_sent_ep)
t.break1 <- 50
tb1 <- ts(pmax(0, t - t.break1), start = 0)

fit.pw <- tslm(ts_sent_ep ~ t + tb1)
t.new <- t[length(t)] + seq(h)
tb1.new <- tb1[length(tb1)] + seq(h)

newdata <- cbind(t=t.new, tb1=tb1.new) %>%
  as.data.frame()
fcasts.pw <- forecast(fit.pw, newdata = newdata)

fit.spline <- tslm(ts_sent_ep ~ t + I(t^2) + I(t^3) +
  I(tb1^3))
fcasts.spl <- forecast(fit.spline, newdata = newdata)

autoplot(ts_all) +
  autolayer(fitted(fit.lin), series = "Linear") +
  autolayer(fitted(fit.exp), series = "Exponential") +
  autolayer(fitted(fit.pw), series = "Piecewise") +
 # autolayer(fitted(fit.spline), series = "Cubic Spline") +
  autolayer(fcasts.pw, series="Piecewise", alpha = 0.5) +
  autolayer(fcasts.lin, series="Linear", PI=FALSE) +
  autolayer(fcasts.exp, series="Exponential", PI=FALSE) +
  #autolayer(fcasts.spl, series="Cubic Spline", alpha = 0.5) + # PI=FALSE) +
  xlab("Episode") + ylab("Proportion of swear words") +
  ggtitle("Fit of predictions of swear-word proportions in GOT") +
  guides(colour = guide_legend(title = " "))

```

husk accuracy scores!!!!!!!!